{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iNaturalist Pre-trained Model",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "copyright",
        "hgAk1LRIezPT"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ian-mcnair/ForageSnap/blob/master/iNaturalist_Pre_trained_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2019 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "khlO4Bu21oZ4",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AlzIlBsScJJ_"
      },
      "source": [
        "# Using Pre-Trained Models Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nTirVS4FWaPx"
      },
      "source": [
        "In this project we will import a pre-existing model that recognizes objects and use the model to identify those objects in a video. We'll edit the video to draw boxes around the identified object and then reassemble the video so that the boxes are shown around objects in the video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7fz_1lEAXeW2"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lbXXnlHyXiKa"
      },
      "source": [
        "### Learning Objectives\n",
        "\n",
        "* Use OpenCV to process images and video.\n",
        "* Use a pre-trained model to identify and label objects in each frame of a video.\n",
        "* Make judgements about classification quality and when to apply predicted labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M62dLnxpX6Le"
      },
      "source": [
        "### Prerequisites\n",
        "\n",
        "* Classification\n",
        "* Saving and Loading Models\n",
        "* OpenCV\n",
        "* Video Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IrPi5LSZYIrw"
      },
      "source": [
        "### Estimated Duration\n",
        "\n",
        "330 minutes (285 minutes working time, 45 minutes for presentations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6RA-x2G2Fd4"
      },
      "source": [
        "### Deliverables\n",
        "\n",
        "1. A copy of this Colab notebook containing your code and responses to the ethical considerations below. The code should produce a functional labeled video.\n",
        "1. A group presentation. After everyone is done, we will ask each group to stand in front of the class and give a brief presentation about what they have done in this lab. The presentation can be a code walkthrough, a group discussion, a slide show, or any other means that conveys what you did over the course of the day and what you learned. If you do create any artifacts for your presentation, please share them in the class folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pzCWjL4DYLgf"
      },
      "source": [
        "### Grading Criteria\n",
        "\n",
        "This project is graded in separate sections that each contribute a percentage of the total score:\n",
        "\n",
        "1. Building and Using a Model (80%)\n",
        "1. Ethical Implications (10%)\n",
        "1. Project Presentation (10%)\n",
        "\n",
        "#### Building and Using a Model\n",
        "\n",
        "There are 6 demonstrations of competency listed in the problem statement below. Each competency is graded on a 3 point scale for a total of 18 available points. The following rubric will be used:\n",
        "\n",
        "| Points | Description |\n",
        "|--------|-------------|\n",
        "| 0      | No attempt at the competency |\n",
        "| 1      | Attempted competency, but in an incorrect manner |\n",
        "| 2      | Attempted competency correctly, but sub-optimally |\n",
        "| 3      | Successful demonstration of competency |\n",
        "\n",
        "\n",
        "#### Ethical Implications\n",
        "\n",
        "There are six questions in the **Ethical Implications** secion. Each question is worth 2 points. The rubric for calculating those points is:\n",
        "\n",
        "| Points | Description |\n",
        "|--------|-------------|\n",
        "| 0      | No attempt at question or answer was off-topic or didn't make sense |\n",
        "| 1      | Question was answered, but answer missed important considerations  |\n",
        "| 2      | Answer adequately considered ethical implications |\n",
        "\n",
        "#### Project Presentation\n",
        "\n",
        "The project presentation will be graded on participation. All members of a team should actively participate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_yACB56w2JVk"
      },
      "source": [
        "## Team\n",
        "\n",
        "Please enter your team members names in the placeholders in this text area:\n",
        "\n",
        "*   *Team Member Placeholder*\n",
        "*   *Team Member Placeholder*\n",
        "*   *Team Member Placeholder*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YTVUYxPwcHhp"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LdIOgOHP1ces"
      },
      "source": [
        "## Exercise 1: Coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jhTEOK1ZmqN8"
      },
      "source": [
        "For this workshop you will process a video frame-by-frame, identify objects in each frame, and draw a bounding box and label around each object.\n",
        " \n",
        "Use the [SSD MobileNet V1 Coco](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) 'ssd_mobilenet_v1_coco' model. The video that you'll be processing can be [found on Pixabay](https://pixabay.com/videos/cars-motorway-speed-motion-traffic-1900/). The 640x360 version of the video is smallest and easiest to handle, though any should work since you must scale down the images for processing.\n",
        " \n",
        "**Graded** demonstrations of competency:\n",
        "1. Obtain the pre-trained model from the [TensorFlow Zoo](https://github.com/tensorflow/models).\n",
        "1. Load the pre-trained model into a TensorFlow object.\n",
        "1. Obtain a video file from Pixabay to use for classification.\n",
        "1. Process the video frame-by-frame creating a modified output video.\n",
        "1. Apply a classification model to an image.\n",
        "1. Draw a bounding box around classified objects in each image.\n",
        " \n",
        "The flow of the program is roughly:\n",
        " \n",
        "* Read in a video file (use the one in this colab if you want)\n",
        "* Load the TensorFlow model\n",
        "* Loop over each frame of the video\n",
        "> * Scale the frame down to a size the model expects\n",
        " * Feed the frame to the model\n",
        " * Loop over detections made by the model\n",
        " >  * If the detection score is above some threshold draw a bounding box onto the frame and put a label in or near the box\n",
        "   * Write the frame back to a new video\n",
        " \n",
        "Some tips:\n",
        " \n",
        "* Processing an entire video is slow, consider truncating the video or skipping over frames during development. Skipping frames will make the video choppy, but you'll be able to see a wider variety of images than you would in a truncated video with all of the original frames in the clip.\n",
        "* The model expects a 300x300 image. You'll likely have to scale your frames to fit the model. When you get a bounding box that box is relative to the scaled image. You'll need to scale the bounding box out to the original image size.\n",
        "* Don't start by trying to process the video. Instead, capture one frame and work with it until you are happy with your object detection, bounding boxes, and labels. Once you get those done worry about video handling.\n",
        "* The [Coco labels file](https://github.com/nightrome/cocostuff/blob/master/labels.txt) can be used to identify classified objects.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7XM35vYWSbim"
      },
      "source": [
        "# Student Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhIQzhXAehiW",
        "colab_type": "text"
      },
      "source": [
        "## Import Statements "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdHozEfdcXtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataframe for \n",
        "import pandas as pd\n",
        "\n",
        "# Video Editing\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Webpage Interaction & Saving\n",
        "import urllib.request\n",
        "import os\n",
        "\n",
        "# Unzipping and Saving Files\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "# Creating the tf Model\n",
        "import tensorflow as tf\n",
        "\n",
        "# Creating a Loading Bar\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NaUhLVle_X8",
        "colab_type": "text"
      },
      "source": [
        "## Get Detection Model Uploaded\n",
        "### Accessing the File via Online Request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEF0EAQ7fHnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
        "file_name = 'faster_rcnn_resnet101_fgvc_2018_07_19.tar.gz'\n",
        "\n",
        "url = base_url + file_name\n",
        "\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "dir_name = file_name[0:-len('.tar.gz')]\n",
        "\n",
        "if os.path.exists(dir_name):\n",
        "  shutil.rmtree(dir_name) \n",
        "\n",
        "tarfile.open(file_name, 'r:gz').extractall('./')\n",
        "\n",
        "os.listdir(dir_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOHYeMW6jcju",
        "colab_type": "text"
      },
      "source": [
        "This just navigates to the correct website to download the model and related files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrHMyXXBhZLz",
        "colab_type": "text"
      },
      "source": [
        "### Downloading and Storing Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpUiUmMeflDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frozen_graph = os.path.join(dir_name, 'frozen_inference_graph.pb')\n",
        "\n",
        "with tf.gfile.GFile(frozen_graph,'rb') as f:\n",
        "  graph_def = tf.GraphDef()\n",
        "  graph_def.ParseFromString(f.read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3J_Y7nTRvKw",
        "colab_type": "text"
      },
      "source": [
        "### Using the model for a single image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4o5TTJFTULY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image = cv.imread('/content/poison.jpg')\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rGRZHI4XEza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04BUw6oSRyIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs = (\n",
        "    'num_detections',\n",
        "    'detection_classes',\n",
        "    'detection_scores',\n",
        "    'detection_boxes' \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMP9rHesSAEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_frame = [image]\n",
        "with tf.Session() as sess:\n",
        "  sess.graph.as_default()\n",
        "  tf.import_graph_def(graph_def, name = '')\n",
        "\n",
        "  detections = sess.run(\n",
        "      [sess.graph.get_tensor_by_name(f'{op}:0') for op in outputs],\n",
        "      feed_dict={ 'image_tensor:0': input_frame }\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CqRt4SvVXLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "detections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0v2pDinVp9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w,h,_ = image.shape\n",
        "for j in range(int(detections[0][0])):\n",
        "\n",
        "  # Bounding Boxes coordinate points\n",
        "  left = int(h * detections[3][0][j][1]) \n",
        "  top = int(w* detections[3][0][j][0]) \n",
        "  right = int(h * detections[3][0][j][3])\n",
        "  bottom = int(w* detections[3][0][j][2])\n",
        "\n",
        "  # Text coordinate point\n",
        "  w_center = int(abs(left - right)/2 -25) + left\n",
        "  h_center = int(top-15)\n",
        "  ind = int(detections[1][0][j])\n",
        "\n",
        "  # Create Rectangle\n",
        "  cv.rectangle(image, (left, top), \n",
        "                      (right, bottom), \n",
        "                      (255, 0, 255), thickness=2)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4pXHN3fjfWK",
        "colab_type": "text"
      },
      "source": [
        "This accesses the model and saves it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2V35fdmWlBh",
        "colab_type": "text"
      },
      "source": [
        "###Preprocessing Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0pKcDyfWsh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#use Keras Preprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgAk1LRIezPT",
        "colab_type": "text"
      },
      "source": [
        "## Processing the Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnH9hT2tjhFZ",
        "colab_type": "text"
      },
      "source": [
        "Getting the original video and its attributes. Creates the output_video which is used to create the final output with the correct squares and labels drawn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTVMhhRKjN8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Outputs needed for detection\n",
        "outputs = (\n",
        "    'num_detections',\n",
        "    'detection_classes',\n",
        "    'detection_scores',\n",
        "    'detection_boxes' \n",
        ")\n",
        "\n",
        "# TQDM is used to create a status bar for those long iterations\n",
        "for i in tqdm(range(0, total_frames , 75)):\n",
        "  \n",
        "  # Accessing Video and grabbing each 75th frame\n",
        "  input_video.set(cv.CAP_PROP_POS_FRAMES, i)\n",
        "  ret, frame = input_video.read()\n",
        "  if not ret:\n",
        "    raise Exception(\"Problem reading frame\", i, \" from video\")\n",
        "\n",
        "  # Frame is thrown into the model and outputs are saved in the detections list\n",
        "  input_frame = [frame]\n",
        "  with tf.Session() as sess:\n",
        "    sess.graph.as_default()\n",
        "    tf.import_graph_def(graph_def, name = '')\n",
        "\n",
        "    detections = sess.run(\n",
        "        [sess.graph.get_tensor_by_name(f'{op}:0') for op in outputs],\n",
        "        feed_dict={ 'image_tensor:0': input_frame }\n",
        "    )\n",
        "  \n",
        "  # Drawing Box and text labels\n",
        "  w,h,_ = frame.shape\n",
        "  for j in range(int(detections[0][0])):\n",
        "    \n",
        "    # Bounding Boxes coordinate points\n",
        "    left = int(h * detections[3][0][j][1]) \n",
        "    top = int(w* detections[3][0][j][0]) \n",
        "    right = int(h * detections[3][0][j][3])\n",
        "    bottom = int(w* detections[3][0][j][2])\n",
        "    \n",
        "    # Text coordinate point\n",
        "    w_center = int(abs(left - right)/2 -25) + left\n",
        "    h_center = int(top-15)\n",
        "    ind = int(detections[1][0][j])\n",
        "    text = labels.iloc[ind,0]\n",
        "    \n",
        "    # Create Rectangle\n",
        "    cv.rectangle(frame, (left, top), \n",
        "                        (right, bottom), \n",
        "                        (255, 0, 255), thickness=2)\n",
        "    # Draws Text\n",
        "    cv.putText(frame, text, \n",
        "               (w_center, h_center), cv.FONT_HERSHEY_COMPLEX, \n",
        "               1.0, [255, 0, 255], 2)\n",
        "  \n",
        "  # Saves frame to output video\n",
        "  output_video.write(frame)\n",
        "  \n",
        "#   Used to check on video, makes it much longer to process\n",
        "#   Use only when needed\n",
        "#   plt.imshow(frame)\n",
        "#   plt.show()\n",
        "  \n",
        "# Removes videos from memory\n",
        "input_video.release()\n",
        "output_video.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYFH97c0eS7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cars = cv.VideoCapture('cars-sampled.mp4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzN-Ggl3jnYh",
        "colab_type": "text"
      },
      "source": [
        "The above two code blocks do quite a bit. The general gist is:\n",
        "- Grab single frame\n",
        "- Run it through model\n",
        "- Use model outputs to draw boxes and text on frame\n",
        "- Save frame to output video\n",
        "- Repeat"
      ]
    }
  ]
}